{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "#################################################\n",
    "import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"# cpus: \", os.cpu_count())\n",
    "NUM_PROCESSES = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63325/63325 [00:52<00:00, 1208.34it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 469628.46it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 417297.55it/s]\n",
      "100%|██████████| 63325/63325 [00:53<00:00, 1179.90it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 1738.43it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 1711.22it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./train.txt\") as f:\n",
    "    train_file_list = f.readlines()\n",
    "with open(\"./val.txt\") as f:\n",
    "    val_file_list = f.readlines()\n",
    "with open(\"./test.txt\") as f:\n",
    "    test_file_list = f.readlines()\n",
    "\n",
    "train_file_list = [x.strip().split(sep=\" \") for x in train_file_list]\n",
    "val_file_list = [x.strip().split(sep=\" \") for x in val_file_list]\n",
    "test_file_list = [x.strip().split(sep=\" \") for x in test_file_list]\n",
    "\n",
    "train_label = [int(x[1]) for x in train_file_list]\n",
    "val_label = [int(x[1]) for x in val_file_list]\n",
    "test_label = [int(x[1]) for x in test_file_list]\n",
    "\n",
    "train_file_list = [x[0] for x in train_file_list]\n",
    "val_file_list = [x[0] for x in val_file_list]\n",
    "test_file_list = [x[0] for x in test_file_list]\n",
    "\n",
    "# print(\"# cpus: \", os.cpu_count())\n",
    "NUM_PROCESSES = 8\n",
    "\n",
    "def ReadImage(filePath):\n",
    "    image = cv2.imread(filePath, cv2.IMREAD_COLOR)\n",
    "    # image = cv2.imread(filePath, cv2.IMREAD_GRAYSCALE)\n",
    "    # image = cv2.resize(image, (256, 256))\n",
    "    return image\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    train_imgs = pool.map(ReadImage, tqdm(train_file_list))\n",
    "    val_imgs = pool.map(ReadImage, tqdm(val_file_list))\n",
    "    test_imgs = pool.map(ReadImage, tqdm(test_file_list))\n",
    "\n",
    "# resize the images to 256x256\n",
    "def ResizeImage(image):\n",
    "    # resized_img = cv2.resize(image, (256, 256))\n",
    "    resized_img = cv2.resize(image, (32, 32))\n",
    "    return resized_img\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    resized_train_imgs = pool.map(ResizeImage, tqdm(train_imgs))\n",
    "    resized_val_imgs = pool.map(ResizeImage, tqdm(val_imgs))\n",
    "    resized_test_imgs = pool.map(ResizeImage, tqdm(test_imgs))\n",
    "\n",
    "##########################################################################################################\n",
    "# def FlattenImages(image: np.ndarray) -> np.ndarray:\n",
    "#     return image.flatten()\n",
    "\n",
    "# with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "#     flatten_resized_train_imgs = np.array(pool.map(FlattenImages, tqdm(resized_train_imgs)))\n",
    "#     flatten_resized_val_imgs = np.array(pool.map(FlattenImages, tqdm(resized_val_imgs)))\n",
    "#     flatten_resized_test_imgs = np.array(pool.map(FlattenImages, tqdm(resized_test_imgs)))\n",
    "\n",
    "\n",
    "# # Ref.: https://github.com/Ixiaohuihuihui/Extract-color-histogram-feature/blob/master/rgb_feature.py\n",
    "# # extract rgb features\n",
    "# def ExtractColorHistFeatures(image):\n",
    "#     features = []\n",
    "#     for channel in range(3):\n",
    "#         hist = cv2.calcHist(images=[image], channels=[channel], mask=None, histSize=[64], ranges=[0,256])\n",
    "#         hist = cv2.normalize(hist, hist)\n",
    "#         # features.extend(hist)\n",
    "#         features.append(hist)\n",
    "#     return features\n",
    "\n",
    "# with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "#     ### tqdm returns an iterator\n",
    "#     # train_features = pool.map(ExtractFeatures, tqdm(resized_train_imgs))\n",
    "#     # val_features = pool.map(ExtractFeatures, tqdm(resized_val_imgs))\n",
    "#     # test_features = pool.map(ExtractFeatures, tqdm(resized_test_imgs))\n",
    "#     train_features = list(tqdm(pool.imap(ExtractColorHistFeatures, resized_train_imgs), total=len(resized_train_imgs)))\n",
    "#     val_features = list(tqdm(pool.imap(ExtractColorHistFeatures, resized_val_imgs), total=len(resized_val_imgs)))\n",
    "#     test_features = list(tqdm(pool.imap(ExtractColorHistFeatures, resized_test_imgs), total=len(resized_test_imgs)))\n",
    "\n",
    "\n",
    "# # flatten and reshape the features into (n_samples, n_features)\n",
    "# train_features = np.array(train_features)\n",
    "# val_features = np.array(val_features)\n",
    "# test_features = np.array(test_features)\n",
    "\n",
    "# def FlattenFeatures(feature):\n",
    "#     return feature.flatten()\n",
    "\n",
    "# with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "#     train_features = np.array(pool.map(FlattenFeatures, tqdm(train_features)))\n",
    "#     val_features = np.array(pool.map(FlattenFeatures, tqdm(val_features)))\n",
    "#     test_features = np.array(pool.map(FlattenFeatures, tqdm(test_features)))\n",
    "\n",
    "# # flatten and reshape the features into (n_samples, n_features)\n",
    "# resized_train_imgs = np.array(resized_train_imgs)\n",
    "# resized_val_imgs = np.array(resized_val_imgs)\n",
    "# resized_test_imgs = np.array(resized_test_imgs)\n",
    "\n",
    "# def FlattenImages(image):\n",
    "#     return image.flatten()\n",
    "\n",
    "# with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "#     resized_train_imgs = np.array(pool.map(FlattenImages, tqdm(resized_train_imgs)))\n",
    "#     resized_val_imgs = np.array(pool.map(FlattenImages, tqdm(resized_val_imgs)))\n",
    "#     resized_test_imgs = np.array(pool.map(FlattenImages, tqdm(resized_test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file=\"./data/resized_train_imgs.npy\", arr=resized_train_imgs)\n",
    "np.save(file=\"./data/resized_val_imgs.npy\", arr=resized_val_imgs)\n",
    "np.save(file=\"./data/resized_test_imgs.npy\", arr=resized_test_imgs)\n",
    "np.save(file=\"./data/train_label\", arr=train_label)\n",
    "np.save(file=\"./data/val_label\", arr=val_label)\n",
    "np.save(file=\"./data/test_label\", arr=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_imgs = np.load(file=\"./data/resized_train_imgs.npy\")\n",
    "resized_val_imgs = np.load(file=\"./data/resized_val_imgs.npy\")\n",
    "resized_test_imgs = np.load(file=\"./data/resized_test_imgs.npy\")\n",
    "train_label = np.load(file=\"./data/train_label.npy\")\n",
    "val_label = np.load(file=\"./data/val_label.npy\")\n",
    "test_label = np.load(file=\"./data/test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63325/63325 [00:02<00:00, 22558.75it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 36884.17it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 39433.33it/s]\n"
     ]
    }
   ],
   "source": [
    "def MoveColorChannel(image: np.ndarray) -> np.ndarray:\n",
    "    return np.moveaxis(image, source=2, destination=0)  # reshape (H, W, C) to (C, H, W)\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    resized_train_imgs = pool.map(MoveColorChannel, tqdm(resized_train_imgs))\n",
    "    resized_val_imgs = pool.map(MoveColorChannel, tqdm(resized_val_imgs))\n",
    "    resized_test_imgs = pool.map(MoveColorChannel, tqdm(resized_test_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shuffle\n",
    "# def shuffle(imgs: list, label: list) -> list:\n",
    "#     random.seed(42)\n",
    "#     idx = np.arange(len(imgs))\n",
    "#     random.shuffle(idx)\n",
    "#     imgs = [imgs[i] for i in idx]\n",
    "#     label = [label[i] for i in idx]\n",
    "#     return imgs, label\n",
    "\n",
    "# resized_train_imgs, train_label = shuffle(resized_train_imgs, train_label)\n",
    "# resized_val_imgs, val_label = shuffle(resized_val_imgs, val_label)\n",
    "# resized_test_imgs, test_label = shuffle(resized_test_imgs, test_label)\n",
    "\n",
    "# convert list to numpy.ndarray\n",
    "resized_train_imgs = np.array(resized_train_imgs)\n",
    "resized_val_imgs = np.array(resized_val_imgs)\n",
    "resized_test_imgs = np.array(resized_test_imgs)\n",
    "# list to numpy.ndarray\n",
    "train_label = np.array(train_label)\n",
    "val_label = np.array(val_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63325, 3, 32, 32)\n",
      "(450, 3, 32, 32)\n",
      "(450, 3, 32, 32)\n",
      "\n",
      "(63325,)\n",
      "(450,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "# check the shape\n",
    "print(resized_train_imgs.shape)\n",
    "print(resized_val_imgs.shape)\n",
    "print(resized_test_imgs.shape)\n",
    "print()\n",
    "print(train_label.shape)\n",
    "print(val_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeOneHot(Y, D_out):\n",
    "    N = Y.shape[0]\n",
    "    Z = np.zeros((N, D_out))\n",
    "    Z[np.arange(N), Y] = 1\n",
    "    return Z\n",
    "\n",
    "def draw_losses(losses):\n",
    "    t = np.arange(len(losses))\n",
    "    plt.plot(t, losses)\n",
    "    plt.show()\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    N = len(X)\n",
    "    i = random.randint(1, N-batch_size)\n",
    "    return X[i:i+batch_size], Y[i:i+batch_size]\n",
    "\n",
    "class FC():\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        #print(\"Build FC\")\n",
    "        self.cache = None\n",
    "        #self.W = {'val': np.random.randn(D_in, D_out), 'grad': 0}\n",
    "        self.W = {'val': np.random.normal(0.0, np.sqrt(2/D_in), (D_in,D_out)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(D_out), 'grad': 0}\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"FC: _forward\")\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"FC: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "        #self._update_params()\n",
    "        return dX\n",
    "\n",
    "    def _update_params(self, lr=0.001):\n",
    "        # Update the parameters\n",
    "        self.W['val'] -= lr*self.W['grad']\n",
    "        self.b['val'] -= lr*self.b['grad']\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build ReLU\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"ReLU: _forward\")\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"ReLU: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX\n",
    "\n",
    "class Sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.cache = X\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X = self.cache\n",
    "        X = self._forward(X)  # 與原作者不同，新增這行\n",
    "        dX = dout*X*(1-X)\n",
    "        return dX\n",
    "\n",
    "class tanh():\n",
    "    \"\"\"\n",
    "    tanh activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = X\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def _backward(self, X):\n",
    "        X = self.cache\n",
    "        dX = dout*(1 - np.tanh(X)**2)\n",
    "        return dX\n",
    "\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Softmax activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build Softmax\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"Softmax: _forward\")\n",
    "        maxes = np.amax(X, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        Y = np.exp(X - maxes)\n",
    "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "        self.cache = (X, Y, Z)\n",
    "        return Z # distribution\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, Y, Z = self.cache\n",
    "        dZ = np.zeros(X.shape)\n",
    "        dY = np.zeros(X.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        N = X.shape[0]\n",
    "        for n in range(N):\n",
    "            i = np.argmax(Z[n])\n",
    "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
    "            M = np.zeros((N,N))\n",
    "            M[:,i] = 1\n",
    "            dY[n,:] = np.eye(N) - M\n",
    "        dX = np.dot(dout,dZ)\n",
    "        dX = np.dot(dX,dY)\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Dropout():\n",
    "    \"\"\"\n",
    "    Dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p=1):\n",
    "        self.cache = None\n",
    "        self.p = p\n",
    "\n",
    "    def _forward(self, X):\n",
    "        M = (np.random.rand(*X.shape) < self.p) / self.p\n",
    "        self.cache = X, M\n",
    "        return X*M\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, M = self.cache\n",
    "        dX = dout*M/self.p\n",
    "        return dX\n",
    "\n",
    "class Conv():\n",
    "    \"\"\"\n",
    "    Conv layer\n",
    "    \"\"\"\n",
    "    def __init__(self, Cin, Cout, F, stride=1, padding=0, bias=True):\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        #self.W = {'val': np.random.randn(Cout, Cin, F, F), 'grad': 0}\n",
    "        self.W = {'val': np.random.normal(0.0,np.sqrt(2/Cin),(Cout,Cin,F,F)), 'grad': 0} # Xavier Initialization\n",
    "        self.b = {'val': np.random.randn(Cout), 'grad': 0}\n",
    "        self.cache = None\n",
    "        self.pad = padding\n",
    "\n",
    "    def _forward(self, X):\n",
    "        X = np.pad(X, ((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)), 'constant')\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        Y = np.zeros((N, self.Cout, H_, W_))\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(self.Cout):\n",
    "                for h in range(H_):\n",
    "                    for w in range(W_):\n",
    "                        Y[n, c, h, w] = np.sum(X[n, :, h:h+self.F, w:w+self.F] * self.W['val'][c, :, :, :]) + self.b['val'][c]\n",
    "\n",
    "        self.cache = X\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        # dout (N,Cout,H_,W_)\n",
    "        # W (Cout, Cin, F, F)\n",
    "        X = self.cache\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        W_rot = np.rot90(np.rot90(self.W['val']))\n",
    "\n",
    "        dX = np.zeros(X.shape)\n",
    "        dW = np.zeros(self.W['val'].shape)\n",
    "        db = np.zeros(self.b['val'].shape)\n",
    "\n",
    "        # dW\n",
    "        for co in range(self.Cout):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(self.F):\n",
    "                    for w in range(self.F):\n",
    "                        dW[co, ci, h, w] = np.sum(X[:,ci,h:h+H_,w:w+W_] * dout[:,co,:,:])\n",
    "\n",
    "        # db\n",
    "        for co in range(self.Cout):\n",
    "            db[co] = np.sum(dout[:,co,:,:])\n",
    "\n",
    "        dout_pad = np.pad(dout, ((0,0),(0,0),(self.F,self.F),(self.F,self.F)), 'constant')\n",
    "        #print(\"dout_pad.shape: \" + str(dout_pad.shape))\n",
    "        # dX\n",
    "        for n in range(N):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(H):\n",
    "                    for w in range(W):\n",
    "                        #print(\"self.F.shape: %s\", self.F)\n",
    "                        #print(\"%s, W_rot[:,ci,:,:].shape: %s, dout_pad[n,:,h:h+self.F,w:w+self.F].shape: %s\" % ((n,ci,h,w),W_rot[:,ci,:,:].shape, dout_pad[n,:,h:h+self.F,w:w+self.F].shape))\n",
    "                        dX[n, ci, h, w] = np.sum(W_rot[:,ci,:,:] * dout_pad[n, :, h:h+self.F,w:w+self.F])\n",
    "\n",
    "        return dX\n",
    "\n",
    "class MaxPool():\n",
    "    def __init__(self, F, stride):\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        # X: (N, Cin, H, W): maxpool along 3rd, 4th dim\n",
    "        (N,Cin,H,W) = X.shape\n",
    "        F = self.F\n",
    "        W_ = int(float(W)/F)\n",
    "        H_ = int(float(H)/F)\n",
    "        Y = np.zeros((N,Cin,W_,H_))\n",
    "        M = np.zeros(X.shape) # mask\n",
    "        for n in range(N):\n",
    "            for cin in range(Cin):\n",
    "                for w_ in range(W_):\n",
    "                    for h_ in range(H_):\n",
    "                        Y[n,cin,w_,h_] = np.max(X[n,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)])\n",
    "                        i,j = np.unravel_index(X[n,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)].argmax(), (F,F))\n",
    "                        M[n,cin,F*w_+i,F*h_+j] = 1\n",
    "        self.cache = M\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        M = self.cache\n",
    "        (N,Cin,H,W) = M.shape\n",
    "        dout = np.array(dout)\n",
    "        #print(\"dout.shape: %s, M.shape: %s\" % (dout.shape, M.shape))\n",
    "        dX = np.zeros(M.shape)\n",
    "        for n in range(N):\n",
    "            for c in range(Cin):\n",
    "                #print(\"(n,c): (%s,%s)\" % (n,c))\n",
    "                dX[n,c,:,:] = dout[n,c,:,:].repeat(2, axis=0).repeat(2, axis=1)\n",
    "        return dX*M\n",
    "\n",
    "def NLLLoss(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N = Y_pred.shape[0]\n",
    "    M = np.sum(Y_pred*Y_true, axis=1)\n",
    "    for e in M:\n",
    "        #print(e)\n",
    "        if e == 0:\n",
    "            loss += 500\n",
    "        else:\n",
    "            loss += -np.log(e)\n",
    "    return loss/N\n",
    "\n",
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        prob = softmax._forward(Y_pred)\n",
    "        loss = NLLLoss(prob, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = prob.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class SoftmaxLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        loss = NLLLoss(Y_pred, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = Y_pred.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class Net(metaclass=ABCMeta):\n",
    "    # Neural network super class\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_params(self, params):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LeNet5(Net):\n",
    "    # LeNet5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv(3, 6, 5)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.pool1 = MaxPool(2,2)\n",
    "        self.conv2 = Conv(6, 16, 5)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.pool2 = MaxPool(2,2)\n",
    "        self.FC1 = FC(16*5*5, 120)\n",
    "        self.ReLU3 = ReLU()\n",
    "        self.FC2 = FC(120, 84)\n",
    "        self.ReLU4 = ReLU()\n",
    "        self.FC3 = FC(84, 50)\n",
    "        self.Softmax = Softmax()\n",
    "\n",
    "        self.p2_shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        p1 = self.pool1._forward(a1)\n",
    "        h2 = self.conv2._forward(p1)\n",
    "        a2 = self.ReLU2._forward(h2)\n",
    "        p2 = self.pool2._forward(a2)\n",
    "        self.p2_shape = p2.shape\n",
    "        fl = p2.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1._forward(fl)\n",
    "        a3 = self.ReLU3._forward(h3)\n",
    "        h4 = self.FC2._forward(a3)\n",
    "        a5 = self.ReLU4._forward(h4)\n",
    "        h5 = self.FC3._forward(a5)\n",
    "        a5 = self.Softmax._forward(h5)\n",
    "        return a5\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dout = self.Softmax._backward(dout)\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.ReLU4._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU3._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "        dout = dout.reshape(self.p2_shape) # reshape\n",
    "        dout = self.pool2._backward(dout)\n",
    "        dout = self.ReLU2._backward(dout)\n",
    "        dout = self.conv2._backward(dout)\n",
    "        dout = self.pool1._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.conv1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])\n",
    "\n",
    "class SGDMomentum():\n",
    "    def __init__(self, params, lr=0.001, momentum=0.99, reg=0):\n",
    "        self.l = len(params)\n",
    "        self.parameters = params\n",
    "        self.velocities = []\n",
    "        for param in self.parameters:\n",
    "            self.velocities.append(np.zeros(param['val'].shape))\n",
    "        self.lr = lr\n",
    "        self.rho = momentum\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(self.l):\n",
    "            self.velocities[i] = self.rho*self.velocities[i] + (1-self.rho)*self.parameters[i]['grad']\n",
    "            self.parameters[i]['val'] -= (self.lr*self.velocities[i] + self.reg*self.parameters[i]['val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data, labels, batch_size=64, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = data.shape[0]\n",
    "        self.num_batches = int(np.ceil(self.num_samples / self.batch_size))\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            self.current_batch = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.indices)\n",
    "            raise StopIteration\n",
    "            \n",
    "        batch_indices = self.indices[self.current_batch*self.batch_size : (self.current_batch+1)*self.batch_size]\n",
    "        batch_data = self.data[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        \n",
    "        self.current_batch += 1\n",
    "        \n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64, D_out: 50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "(1) Prepare Data: Load, Shuffle, Normalization, Batching, Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = resized_train_imgs, train_label, resized_test_imgs, test_label\n",
    "X_train, X_test = X_train/float(255), X_test/float(255)\n",
    "X_train -= np.mean(X_train)\n",
    "X_test -= np.mean(X_test)\n",
    "\n",
    "batch_size = 64\n",
    "# D_in = 784\n",
    "D_out = 50\n",
    "\n",
    "# print(\"batch_size: \" + str(batch_size) + \", D_in: \" + str(D_in) + \", D_out: \" + str(D_out))\n",
    "print(\"batch_size: \" + str(batch_size) + \", D_out: \" + str(D_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [2:26:05<00:00,  8.85s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0% epoch: 1, train loss: 3.9124\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [2:25:26<00:00,  8.82s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0% epoch: 2, train loss: 3.9119\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [2:24:18<00:00,  8.75s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0% epoch: 3, train loss: 3.9119\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [2:21:36<00:00,  8.58s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0% epoch: 4, train loss: 3.9119\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [2:25:36<00:00,  8.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% epoch: 5, train loss: 3.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Lenet Forward Test ###\n",
    "model = LeNet5()\n",
    "dataloader = DataLoader(data=resized_train_imgs,\n",
    "                        labels=train_label,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True)\n",
    "\n",
    "losses = []\n",
    "optim = SGDMomentum(params=model.get_params(), lr=1e-3, momentum=0.99, reg=0)\n",
    "# optim = SGD(params=model.get_params(), lr=1e-3, reg=0)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "EPOCHS = 5\n",
    "for i in range(EPOCHS):\n",
    "    losses_epoch = 0\n",
    "    print(f\"epoch: {i+1}\")\n",
    "    with tqdm(total=dataloader.num_batches) as pbar:\n",
    "        for X_batch, Y_batch in dataloader:\n",
    "            # get batch, make onehot\n",
    "            # X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)\n",
    "            Y_batch = MakeOneHot(Y_batch, D_out)\n",
    "\n",
    "            # forward, loss, backward, step\n",
    "            Y_pred = model.forward(X_batch)\n",
    "            loss, _ = criterion.get(Y_pred, Y_batch)  # loss, dout\n",
    "            dout = Y_pred - Y_batch  # pred - label\n",
    "            model.backward(dout)\n",
    "            optim.step()\n",
    "\n",
    "            losses_epoch += loss\n",
    "            pbar.update(1)\n",
    "\n",
    "        losses_epoch /= dataloader.num_batches\n",
    "            \n",
    "    if i % 1 == 0:\n",
    "        print(\"%s%% epoch: %s, train loss: %s\" % (round(100*(i+1)/EPOCHS, 4), i+1, round(losses_epoch, 4)))\n",
    "        losses.append(losses_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16516/370755558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save loss, accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./loss/sdgm_train_losses.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./loss/sgdm_val_losses.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# save params\n",
    "weights = model.get_params()\n",
    "with open(\"sgdm_weights.pkl\",\"wb\") as f:\n",
    "\tpickle.dump(weights, f)\n",
    "\n",
    "# save loss, accuracy\n",
    "with open(\"./loss/sdgm_train_losses.txt\", \"w\") as fp:\n",
    "\tjson.dump(train_losses, fp)\n",
    "with open(\"./loss/sgdm_val_losses.txt\", \"w\") as fp:\n",
    "\tjson.dump(val_losses, fp)\n",
    "with open(\"./acc/sgdm_train_accuracy.txt\", \"w\") as fp:\n",
    "\tjson.dump(train_accuracy, fp)\n",
    "with open(\"./acc/sgdm_val_accuracy.txt\", \"w\") as fp:\n",
    "\tjson.dump(val_accuracy, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_losses(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET ACC\n",
    "Y_pred = model.forward(X_train)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_train\n",
    "result = list(result)\n",
    "print(\"TRAIN--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_train.shape[0]) + \", acc=\" + str(result.count(0)/X_train.shape[0]))\n",
    "\n",
    "# TEST SET ACC\n",
    "Y_pred = model.forward(X_test)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_test\n",
    "result = list(result)\n",
    "print(\"TEST--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_test.shape[0]) + \", acc=\" + str(result.count(0)/X_test.shape[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
