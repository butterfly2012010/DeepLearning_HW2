{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# For Perceptron\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63325/63325 [00:58<00:00, 1078.67it/s]  \n",
      "100%|██████████| 450/450 [00:00<00:00, 201112.07it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 222260.57it/s]\n",
      "100%|██████████| 63325/63325 [01:06<00:00, 946.40it/s] \n",
      "100%|██████████| 450/450 [00:00<00:00, 1254.42it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 1476.69it/s]\n",
      "100%|██████████| 63325/63325 [00:23<00:00, 2663.26it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 2418.60it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 2674.58it/s]\n",
      "100%|██████████| 63325/63325 [00:00<00:00, 65869.41it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 28939.10it/s]\n",
      "100%|██████████| 450/450 [00:00<00:00, 30556.87it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./train.txt\") as f:\n",
    "    train_file_list = f.readlines()\n",
    "with open(\"./val.txt\") as f:\n",
    "    val_file_list = f.readlines()\n",
    "with open(\"./test.txt\") as f:\n",
    "    test_file_list = f.readlines()\n",
    "\n",
    "train_file_list = [x.strip().split(sep=\" \") for x in train_file_list]\n",
    "val_file_list = [x.strip().split(sep=\" \") for x in val_file_list]\n",
    "test_file_list = [x.strip().split(sep=\" \") for x in test_file_list]\n",
    "\n",
    "train_label = [int(x[1]) for x in train_file_list]\n",
    "val_label = [int(x[1]) for x in val_file_list]\n",
    "test_label = [int(x[1]) for x in test_file_list]\n",
    "\n",
    "train_file_list = [x[0] for x in train_file_list]\n",
    "val_file_list = [x[0] for x in val_file_list]\n",
    "test_file_list = [x[0] for x in test_file_list]\n",
    "\n",
    "# print(\"# cpus: \", os.cpu_count())\n",
    "NUM_PROCESSES = 8\n",
    "\n",
    "def ReadImage(filePath):\n",
    "    image = cv2.imread(filePath, cv2.IMREAD_COLOR)\n",
    "    # image = cv2.imread(filePath, cv2.IMREAD_GRAYSCALE)\n",
    "    # image = cv2.resize(image, (256, 256))\n",
    "    return image\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    train_imgs = pool.map(ReadImage, tqdm(train_file_list))\n",
    "    val_imgs = pool.map(ReadImage, tqdm(val_file_list))\n",
    "    test_imgs = pool.map(ReadImage, tqdm(test_file_list))\n",
    "\n",
    "# resize the images to 256x256\n",
    "def ResizeImage(image):\n",
    "    # resized_img = cv2.resize(image, (256, 256))\n",
    "    resized_img = cv2.resize(image, (64, 64))\n",
    "    return resized_img\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    resized_train_imgs = pool.map(ResizeImage, tqdm(train_imgs))\n",
    "    resized_val_imgs = pool.map(ResizeImage, tqdm(val_imgs))\n",
    "    resized_test_imgs = pool.map(ResizeImage, tqdm(test_imgs))\n",
    "\n",
    "\n",
    "# Ref.: https://github.com/Ixiaohuihuihui/Extract-color-histogram-feature/blob/master/rgb_feature.py\n",
    "# extract rgb features\n",
    "def ExtractColorHistFeatures(image):\n",
    "    features = []\n",
    "    for channel in range(3):\n",
    "        hist = cv2.calcHist(images=[image], channels=[channel], mask=None, histSize=[64], ranges=[0,256])\n",
    "        hist = cv2.normalize(hist, hist)\n",
    "        # features.extend(hist)\n",
    "        features.append(hist)\n",
    "    return features\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    ### tqdm returns an iterator\n",
    "    # train_features = pool.map(ExtractFeatures, tqdm(resized_train_imgs))\n",
    "    # val_features = pool.map(ExtractFeatures, tqdm(resized_val_imgs))\n",
    "    # test_features = pool.map(ExtractFeatures, tqdm(resized_test_imgs))\n",
    "    train_features = list(tqdm(pool.imap(ExtractColorHistFeatures, resized_train_imgs), total=len(resized_train_imgs)))\n",
    "    val_features = list(tqdm(pool.imap(ExtractColorHistFeatures, resized_val_imgs), total=len(resized_val_imgs)))\n",
    "    test_features = list(tqdm(pool.imap(ExtractColorHistFeatures, resized_test_imgs), total=len(resized_test_imgs)))\n",
    "\n",
    "\n",
    "# flatten and reshape the features into (n_samples, n_features)\n",
    "train_features = np.array(train_features)\n",
    "val_features = np.array(val_features)\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "def FlattenFeatures(feature):\n",
    "    return feature.flatten()\n",
    "\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    train_features = np.array(pool.map(FlattenFeatures, tqdm(train_features)))\n",
    "    val_features = np.array(pool.map(FlattenFeatures, tqdm(val_features)))\n",
    "    test_features = np.array(pool.map(FlattenFeatures, tqdm(test_features)))\n",
    "\n",
    "# # flatten and reshape the features into (n_samples, n_features)\n",
    "# resized_train_imgs = np.array(resized_train_imgs)\n",
    "# resized_val_imgs = np.array(resized_val_imgs)\n",
    "# resized_test_imgs = np.array(resized_test_imgs)\n",
    "\n",
    "# def FlattenImages(image):\n",
    "#     return image.flatten()\n",
    "\n",
    "# with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "#     resized_train_imgs = np.array(pool.map(FlattenImages, tqdm(resized_train_imgs)))\n",
    "#     resized_val_imgs = np.array(pool.map(FlattenImages, tqdm(resized_val_imgs)))\n",
    "#     resized_test_imgs = np.array(pool.map(FlattenImages, tqdm(resized_test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(label, n_classes):\n",
    "    enc = np.zeros(shape=(len(label), n_classes))\n",
    "    for idx, val in enumerate(label):\n",
    "        enc[idx, val] = 1\n",
    "    return enc\n",
    "\n",
    "train_label = onehot(np.array(train_label), 50)\n",
    "val_label = onehot(np.array(val_label), 50)\n",
    "test_label = onehot(np.array(test_label), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize the input of perceptron\n",
    "# normalized_resized_train_imgs = resized_train_imgs / 255\n",
    "# normalized_resized_val_imgs = resized_val_imgs / 255\n",
    "# normalized_resized_test_imgs = resized_test_imgs / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data, labels, batch_size=32, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = data.shape[0]\n",
    "        self.num_batches = int(np.ceil(self.num_samples / self.batch_size))\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            self.current_batch = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.indices)\n",
    "            raise StopIteration\n",
    "            \n",
    "        batch_indices = self.indices[self.current_batch*self.batch_size : (self.current_batch+1)*self.batch_size]\n",
    "        batch_data = self.data[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        \n",
    "        self.current_batch += 1\n",
    "        \n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_features, train_label, shuffle=True)\n",
    "val_dataloader = DataLoader(val_features, val_label, shuffle=True)\n",
    "test_dataloader = DataLoader(test_features, test_label, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.limit = 1 / math.sqrt(input_size)\n",
    "        # self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights1 = np.random.uniform(low=-self.limit, high=self.limit, size=(self.input_size, self.hidden_size))\n",
    "        self.biases1 = np.zeros((1, self.hidden_size))\n",
    "        # self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.weights2 = np.random.uniform(low=-self.limit, high=self.limit, size=(self.hidden_size, self.output_size))\n",
    "        self.biases2 = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        self.z1 = np.dot(X, self.weights1) + self.biases1\n",
    "        self.a1 = np.exp(self.z1) / np.sum(np.exp(self.z1), axis=1, keepdims=True)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.biases2\n",
    "        self.a2 = np.exp(self.z2) / np.sum(np.exp(self.z2), axis=1, keepdims=True)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Compute error\n",
    "        delta3 = self.a2 - y\n",
    "        delta2 = np.dot(delta3, self.weights2.T) * (self.a1 * (1 - self.a1))\n",
    "        \n",
    "        # Compute gradients\n",
    "        d_weights2 = np.dot(self.a1.T, delta3)\n",
    "        d_biases2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        d_weights1 = np.dot(X.T, delta2)\n",
    "        d_biases1 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights2 -= learning_rate * d_weights2\n",
    "        self.biases2 -= learning_rate * d_biases2\n",
    "        self.weights1 -= learning_rate * d_weights1\n",
    "        self.biases1 -= learning_rate * d_biases1\n",
    "        \n",
    "    def train(self, dataloader, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for X, y in dataloader:\n",
    "                # Forward pass\n",
    "                output = self.forward(X)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(X, y, learning_rate)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = np.mean(-np.sum(y * np.log(output), axis=1))\n",
    "                \n",
    "            # Print progress\n",
    "            if epoch % 1 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Train Loss {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        output = self.forward(X)\n",
    "        \n",
    "        # Return predicted class\n",
    "        return np.argmax(output, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss 3.9048\n",
      "Epoch 2, Train Loss 3.9259\n",
      "Epoch 3, Train Loss 3.9019\n",
      "Epoch 4, Train Loss 3.9008\n",
      "Epoch 5, Train Loss 3.9080\n",
      "Epoch 6, Train Loss 3.8994\n",
      "Epoch 7, Train Loss 3.9112\n",
      "Epoch 8, Train Loss 3.9075\n",
      "Epoch 9, Train Loss 3.9000\n",
      "Epoch 10, Train Loss 3.9007\n",
      "Epoch 11, Train Loss 3.9060\n",
      "Epoch 12, Train Loss 3.9196\n",
      "Epoch 13, Train Loss 3.9068\n",
      "Epoch 14, Train Loss 3.9061\n",
      "Epoch 15, Train Loss 3.9014\n",
      "Epoch 16, Train Loss 3.9168\n",
      "Epoch 17, Train Loss 3.9061\n",
      "Epoch 18, Train Loss 3.9000\n",
      "Epoch 19, Train Loss 3.9301\n",
      "Epoch 20, Train Loss 3.9454\n",
      "Epoch 21, Train Loss 3.9016\n",
      "Epoch 22, Train Loss 3.9008\n",
      "Epoch 23, Train Loss 3.9001\n",
      "Epoch 24, Train Loss 3.9021\n",
      "Epoch 25, Train Loss 3.9017\n",
      "Epoch 26, Train Loss 3.9144\n",
      "Epoch 27, Train Loss 3.9181\n",
      "Epoch 28, Train Loss 3.9003\n",
      "Epoch 29, Train Loss 3.8949\n",
      "Epoch 30, Train Loss 3.9205\n"
     ]
    }
   ],
   "source": [
    "perceptron = TwoLayerPerceptron(input_size=train_features.shape[1],\n",
    "                                hidden_size=32,\n",
    "                                output_size=50)\n",
    "\n",
    "# Train model\n",
    "perceptron.train(dataloader=train_dataloader, learning_rate=1e-3, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0311\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = perceptron.predict(test_features)\n",
    "accuracy = np.mean(y_pred == np.argmax(test_label, axis=1))\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
